# Learning to Learn without Forgetting using Attention

This repository is the official implementation of *Learning to Learn without Forgetting using Attention*.

## Available datasets

SplitMNIST.

RotatedMNIST.

SplitCIFAR-100.

## Usage

To run the code, use *main_splitmnist.py*, *main_rotatedmnist.py*, *main_cifar100.py*.

These codes will train the model and compute the average accuracy, the backward transfer (BWT), and the forward transfer (FWT) metrics.

## Notes
This code has been tested with Python 3.10.12 and PyTorch 1.13.1 with CUDA 11.7.